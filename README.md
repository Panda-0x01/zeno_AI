# Zeno - Local AI Desktop Assistant

A cross-platform desktop AI assistant powered by Ollama, featuring voice I/O, local-first privacy, and extensible plugin system.

## ğŸ—ï¸ Architecture

```
React (TypeScript) + Vite
         â†“
    Electron Shell
         â†“
   WebSocket (secure)
         â†“
  Python FastAPI Backend
         â†“
    Ollama Local API
```

## âœ¨ Features

- ğŸ¤– **Ollama Integration**: Full model management, streaming responses, context window control
- ğŸ¤ **Voice I/O**: Web Speech API + optional offline STT (Whisper/VOSK), TTS with multiple voices
- ğŸ” **Security First**: Local-only by default, encrypted storage, sandboxed execution, audit logging
- ğŸ¨ **Polished UI**: Light/dark themes, accessibility (ARIA, screen readers), keyboard shortcuts
- ğŸ”Œ **Plugin System**: User-defined commands with permission controls
- ğŸ“¦ **Cross-Platform**: Windows, macOS, Linux installers

## ğŸš€ Quick Start

### Prerequisites

1. **Ollama** (required): [Install Ollama](https://ollama.ai/download)
   ```bash
   # After installation, pull a model:
   ollama pull llama2
   ```

2. **Node.js 18+**: [Download](https://nodejs.org/)

3. **Python 3.10+**: [Download](https://www.python.org/downloads/)

### Development Setup

1. **Clone and install dependencies**:
   ```bash
   git clone <repo-url>
   cd jarvis
   npm install
   ```

2. **Setup Python backend**:
   ```bash
   cd backend
   python -m venv venv
   
   # Windows
   venv\Scripts\activate
   
   # macOS/Linux
   source venv/bin/activate
   
   pip install -r requirements.txt
   ```

3. **Configure environment**:
   ```bash
   # Copy example configs
   cp backend/.env.example backend/.env
   cp frontend/.env.example frontend/.env
   ```

4. **Run in development mode**:
   ```bash
   # Terminal 1 - Backend
   cd backend
   venv\Scripts\activate  # or source venv/bin/activate
   python main.py
   
   # Terminal 2 - Frontend + Electron
   npm run dev
   ```

### Production Build

```bash
# Build all platforms (requires platform-specific tools)
npm run build:all

# Build for current platform only
npm run build

# Outputs in /dist folder
```

## ğŸ“ Project Structure

```
jarvis/
â”œâ”€â”€ frontend/           # React + Vite + TypeScript
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/ # UI components
â”‚   â”‚   â”œâ”€â”€ services/   # Backend communication
â”‚   â”‚   â”œâ”€â”€ hooks/      # React hooks
â”‚   â”‚   â””â”€â”€ types/      # TypeScript definitions
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ backend/            # Python FastAPI
â”‚   â”œâ”€â”€ api/            # API routes
â”‚   â”œâ”€â”€ services/       # Business logic (Ollama, STT, TTS)
â”‚   â”œâ”€â”€ security/       # Auth, encryption, sandboxing
â”‚   â”œâ”€â”€ plugins/        # Plugin system
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ electron/           # Electron main process
â”‚   â”œâ”€â”€ main.js         # App lifecycle
â”‚   â”œâ”€â”€ preload.js      # Secure IPC bridge
â”‚   â””â”€â”€ tray.js         # System tray
â”œâ”€â”€ scripts/            # Build and packaging scripts
â”œâ”€â”€ tests/              # Integration tests
â””â”€â”€ docs/               # Additional documentation
```

## ğŸ”§ Configuration

### Backend Configuration (`backend/.env`)

```env
# Ollama
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_MODEL=llama2
MAX_CONTEXT_TOKENS=4096

# Server
BACKEND_HOST=127.0.0.1
BACKEND_PORT=8765
WS_SECRET_TOKEN=<auto-generated>

# Security
ENABLE_ENCRYPTION=true
AUDIT_LOG_ENABLED=true
REQUIRE_ACTION_CONFIRMATION=true

# STT/TTS
STT_ENGINE=web  # web, whisper, vosk
TTS_ENGINE=web  # web, coqui, pyttsx3
WAKE_WORD_ENABLED=false
```

### Model Selection

Recommended models by use case:
- **Fast responses**: `llama2:7b`, `mistral:7b`
- **Better quality**: `llama2:13b`, `mixtral:8x7b`
- **Code assistance**: `codellama:13b`, `deepseek-coder:6.7b`

## ğŸ¤ Voice Features

### Web Speech API (Default)
- Works out of the box in Chromium-based Electron
- Requires internet for some browsers
- Push-to-talk and continuous listening modes

### Offline STT (Optional)

**Whisper (Recommended)**:
```bash
cd backend
pip install openai-whisper
# Downloads model on first use (~1.5GB for base model)
```

**VOSK (Lightweight)**:
```bash
pip install vosk
# Download model: https://alphacephei.com/vosk/models
# Extract to backend/models/vosk/
```

### Wake Word (Optional)

**Porcupine**:
```bash
pip install pvporcupine
# Free tier: 1 wake word, requires API key
# Set PORCUPINE_ACCESS_KEY in .env
```

## ğŸ” Security & Privacy

### Threat Model
- **Local-first**: All data stays on your machine by default
- **No telemetry**: Zero analytics or tracking
- **Encrypted storage**: Optional AES-256-GCM encryption for chat history
- **Sandboxed execution**: User scripts run in restricted environment
- **Audit logging**: All actions logged with timestamps

### Data Storage
- Chat history: `~/.jarvis/history.db` (SQLite)
- Audit logs: `~/.jarvis/logs/`
- Encrypted with password-derived key (PBKDF2 + AES-GCM)

### Action Permissions
All system actions require explicit user confirmation:
- Shell command execution
- File system access
- Network requests
- Application launching

## ğŸ§ª Testing

```bash
# Frontend tests
cd frontend
npm test

# Backend tests
cd backend
pytest

# Integration tests
npm run test:integration

# E2E tests
npm run test:e2e
```

## ğŸ“¦ Building Installers

### Windows
```bash
npm run build:win
# Outputs: dist/JARVIS-Setup-1.0.0.exe (NSIS)
```

### macOS
```bash
npm run build:mac
# Outputs: dist/JARVIS-1.0.0.dmg
# Note: Requires code signing for notarization
```

### Linux
```bash
npm run build:linux
# Outputs: dist/JARVIS-1.0.0.AppImage, .deb, .rpm
```

## ğŸ”Œ Plugin System

Create custom commands in `~/.jarvis/plugins/`:

```python
# example_plugin.py
from jarvis.plugin import Plugin, command

class WeatherPlugin(Plugin):
    @command(name="weather", description="Get weather info")
    async def get_weather(self, location: str):
        # Your implementation
        return f"Weather in {location}: Sunny"
```

Register in settings UI or `plugins.json`.

## ğŸ¨ Keyboard Shortcuts

- `Ctrl/Cmd + Shift + J`: Toggle main window
- `Ctrl/Cmd + K`: Focus chat input
- `Ctrl/Cmd + ,`: Open settings
- `Space` (hold): Push-to-talk
- `Esc`: Stop generation

## ğŸ› Troubleshooting

### Ollama Connection Failed
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

### Python Backend Won't Start
```bash
# Check port availability
netstat -an | findstr 8765  # Windows
lsof -i :8765               # macOS/Linux

# Verify Python dependencies
pip install -r backend/requirements.txt --upgrade
```

### Electron Build Fails
```bash
# Clear cache
npm run clean
rm -rf node_modules
npm install
```

## ğŸ“š Documentation

- [Architecture Deep Dive](docs/ARCHITECTURE.md)
- [Security Whitepaper](docs/SECURITY.md)
- [Plugin Development Guide](docs/PLUGINS.md)
- [API Reference](docs/API.md)
- [Contributing Guidelines](CONTRIBUTING.md)

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file

## ğŸ™ Third-Party Licenses

- **Ollama**: MIT License
- **FastAPI**: MIT License
- **React**: MIT License
- **Electron**: MIT License
- **Whisper** (optional): MIT License
- **VOSK** (optional): Apache 2.0
- **Porcupine** (optional): Proprietary (free tier available)

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

## âš ï¸ Disclaimer

This is a local-first AI assistant. While we prioritize security and privacy, users are responsible for:
- Securing their encryption passwords
- Reviewing plugin code before installation
- Understanding model capabilities and limitations
- Complying with Ollama and model licenses
